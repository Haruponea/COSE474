{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvfl_DbclIyT"
      },
      "source": [
        "# Implementing a Recurrent Neural Network for Sequential Data\n",
        "\n",
        "In this exercise we will develop a recurrent neural network with vanilla RNN and GRU to perform classification, and test it out on the Text Document Classification Dataset. ðŸ˜€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I82oSCDhl543"
      },
      "source": [
        "## Loading Text Document Classification Dataset.\n",
        "\n",
        "Dataset contains different categories of text data. It contains labels for five different categories.\n",
        "Politics = 0, Sport = 1, Technology = 2, Entertainment = 3, Business = 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m95kJBSNnoUt",
        "outputId": "862cd786-7087-41a7-fb32-c939f9ee64de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2225, 2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# load dataset\n",
        "data = pd.read_csv(\"./df_file.csv\")\n",
        "print(data.shape)\n",
        "\n",
        "def preprocess_data(data, text_column, label_column):\n",
        "    texts = data[text_column].values\n",
        "    labels = data[label_column].values\n",
        "    return texts, labels\n",
        "\n",
        "texts, labels = preprocess_data(data, 'Text', 'Label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMIyNiD1W3iC",
        "outputId": "e7fca900-5646-4d4b-d547-30263c7b0de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2225, 100)\n"
          ]
        }
      ],
      "source": [
        "# Build Tokenizer\n",
        "tokenizer = Tokenizer(num_words=5000)  #num_words : number of word dictionary\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# padding for sentence length\n",
        "max_seq_length = 100\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length)\n",
        "\n",
        "print(padded_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsWaEmAgYk8z",
        "outputId": "28a9d220-6934-42c2-c89e-21512ebc8fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([32, 100])\n",
            "Target batch shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# split dataset\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(padded_sequences, labels, test_size=0.3, random_state=42)\n",
        "valid_features, test_features, valid_labels, test_labels = train_test_split(test_features, test_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# change into tensor\n",
        "train_features = torch.tensor(train_features, dtype=torch.long)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "valid_features = torch.tensor(valid_features, dtype=torch.long)\n",
        "valid_labels = torch.tensor(valid_labels, dtype=torch.long)\n",
        "test_features = torch.tensor(test_features, dtype=torch.long)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Build DataLoader\n",
        "train_dataset = TensorDataset(train_features, train_labels)\n",
        "valid_dataset = TensorDataset(valid_features, valid_labels)\n",
        "test_dataset = TensorDataset(test_features, test_labels)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# print batch size\n",
        "for batch in train_loader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Input batch shape: {inputs.shape}\")\n",
        "    print(f\"Target batch shape: {targets.shape}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFn85HMEGsY0"
      },
      "source": [
        "Code for testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JXjB8Q8vtLg-"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch[0].to(device)\n",
        "            labels = batch[1].to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "    return avg_loss\n",
        "\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = batch[0].to(device)\n",
        "            labels = batch[1].to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tmviyweFVD1"
      },
      "source": [
        "## Vanilla RNN Implementation\n",
        "\n",
        "Here we will implement vanilla RNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "911_8XSIFUBZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VanillaRNN(nn.Module):\n",
        "    #####fill in the blanks#####\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "        # Weights for input to hidden connection\n",
        "        self.Wx = nn.Linear(embedding_dim, hidden_size)\n",
        "        # Weights for hidden to hidden connection\n",
        "        self.Wh = nn.Linear(hidden_size, hidden_size)\n",
        "        # Fully connected layer to map hidden state to output\n",
        "        self.fc = nn.Linear(hidden_size,output_size)\n",
        "\n",
        "        # Activation function (tanh) for the hidden state\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Initialize hidden state with zeros\n",
        "        h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "        # Iterate over each time step\n",
        "        for t in range(seq_len):\n",
        "\n",
        "            xt = x[:, t, :]##fill in your code##  # Select the t-th time step input\n",
        "\n",
        "            h = self.tanh(self.Wx(xt)+self.Wh(h))##fill in your code##  # Update hidden state\n",
        "\n",
        "        # Use the hidden state from the last time step to predict the output\n",
        "        out = self.fc(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3G66ByEGF1Dl"
      },
      "outputs": [],
      "source": [
        "# model params\n",
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "hidden_size = 128\n",
        "output_size = 5\n",
        "\n",
        "# build GRU model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VanillaRNN(vocab_size, embedding_dim, hidden_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYu1c-HEIjh0"
      },
      "source": [
        "Here we will train and test vanilla RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "u9JzMMZ5GmW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4286f71-951c-43cd-9e5a-3136c686cd82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.6106\n",
            "Validation Loss: 1.5695, Accuracy: 31.74%\n",
            "Best model saved with validation loss: 1.5695\n",
            "Epoch [2/10], Loss: 1.4377\n",
            "Validation Loss: 1.5870, Accuracy: 32.34%\n",
            "Epoch [3/10], Loss: 1.3382\n",
            "Validation Loss: 1.6202, Accuracy: 33.83%\n",
            "Epoch [4/10], Loss: 1.3794\n",
            "Validation Loss: 1.6800, Accuracy: 31.14%\n",
            "Epoch [5/10], Loss: 1.1629\n",
            "Validation Loss: 1.7420, Accuracy: 35.03%\n",
            "Epoch [6/10], Loss: 0.6885\n",
            "Validation Loss: 1.7424, Accuracy: 36.53%\n",
            "Epoch [7/10], Loss: 0.7030\n",
            "Validation Loss: 1.8543, Accuracy: 36.83%\n",
            "Epoch [8/10], Loss: 0.4302\n",
            "Validation Loss: 1.9784, Accuracy: 37.72%\n",
            "Epoch [9/10], Loss: 0.5231\n",
            "Validation Loss: 2.0196, Accuracy: 41.92%\n",
            "Epoch [10/10], Loss: 0.2799\n",
            "Validation Loss: 2.1882, Accuracy: 38.02%\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#start training\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "    val_loss = evaluate_model(model, valid_loader)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'rnn_best_model.pth')\n",
        "        print(f\"Best model saved with validation loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXw_So68IrMw"
      },
      "source": [
        "Here you can get your final test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2UmhMXcsG-Va",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d256208f-5a64-4db8-a71b-b106c5caf264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test set:\n",
            "Test Loss: 1.8908, Accuracy: 43.11%\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating on test set:\")\n",
        "test_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXSOwA3KuCGo"
      },
      "source": [
        "## GRU (Gated Recurrent Unit) Implementation\n",
        "\n",
        "Here, we will implement GRU model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "oTNCyu7tlDnS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "  #####fill in the blanks#####\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Update gate\n",
        "        self.W_z = nn.Linear(input_size, hidden_size)\n",
        "        self.U_z = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Reset gate\n",
        "        self.W_r = nn.Linear(input_size, hidden_size)\n",
        "        self.U_r = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Candidate hidden state\n",
        "        self.W_h = nn.Linear(input_size, hidden_size)\n",
        "        self.U_h = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        # Update gate\n",
        "        # hint : The update gate controls how much of the previous hidden state (h_prev)\n",
        "        # should be carried forward to the next hidden state.\n",
        "        z_t = torch.sigmoid(self.W_z(x)+ self.U_z(h_prev))##fill in your code##\n",
        "\n",
        "        # Reset gate\n",
        "        # hint : The reset gate determines how much of the previous hidden state\n",
        "        # should be \"reset\" or ignored when computing the candidate hidden state\n",
        "        r_t = torch.sigmoid(self.W_r(x)+ self.U_r(h_prev))##fill in your code##\n",
        "\n",
        "        # Candidate hidden state\n",
        "        # hint : The candidate hidden state is computed using a combination of the reset hidden state and the current input.\n",
        "        h_tilde = torch.tanh(self.W_h(x)+ self.U_h(r_t*h_prev))##fill in your code##\n",
        "\n",
        "        # New final hidden state\n",
        "        # The final hidden state is a blend of the previous hidden state and the candidate hidden state\n",
        "        h_t = (1-z_t)*h_prev + z_t*h_tilde##fill in your code##\n",
        "\n",
        "        return h_t\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "        self.gru_cell = GRUCell(embedding_dim, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            xt = x[:, t, :]\n",
        "            h_t = self.gru_cell.forward(xt,h_t)##fill in your code##\n",
        "\n",
        "        out = self.fc(h_t)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VciDOg7yueId"
      },
      "outputs": [],
      "source": [
        "# model params\n",
        "vocab_size = 5000\n",
        "embedding_dim = 64\n",
        "hidden_size = 128\n",
        "output_size = 5\n",
        "\n",
        "# build GRU model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GRUModel(vocab_size, embedding_dim, hidden_size, output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQbvnUd2MY0"
      },
      "source": [
        "Here we will train and test our GRU model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TrJWCBlFnEos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52f2db8-c53e-4d9b-9d60-7ad0f16f405a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.6492\n",
            "Validation Loss: 1.5540, Accuracy: 32.04%\n",
            "Best model saved with validation loss: 1.5540\n",
            "Epoch [2/20], Loss: 1.4387\n",
            "Validation Loss: 1.4948, Accuracy: 35.93%\n",
            "Best model saved with validation loss: 1.4948\n",
            "Epoch [3/20], Loss: 0.9705\n",
            "Validation Loss: 1.2335, Accuracy: 49.10%\n",
            "Best model saved with validation loss: 1.2335\n",
            "Epoch [4/20], Loss: 1.0974\n",
            "Validation Loss: 1.1403, Accuracy: 54.19%\n",
            "Best model saved with validation loss: 1.1403\n",
            "Epoch [5/20], Loss: 0.6026\n",
            "Validation Loss: 0.9910, Accuracy: 66.77%\n",
            "Best model saved with validation loss: 0.9910\n",
            "Epoch [6/20], Loss: 0.7683\n",
            "Validation Loss: 0.9342, Accuracy: 68.26%\n",
            "Best model saved with validation loss: 0.9342\n",
            "Epoch [7/20], Loss: 0.1823\n",
            "Validation Loss: 0.9703, Accuracy: 65.57%\n",
            "Epoch [8/20], Loss: 0.2579\n",
            "Validation Loss: 1.0176, Accuracy: 63.47%\n",
            "Epoch [9/20], Loss: 0.1193\n",
            "Validation Loss: 0.9444, Accuracy: 68.26%\n",
            "Epoch [10/20], Loss: 0.0561\n",
            "Validation Loss: 0.9176, Accuracy: 72.46%\n",
            "Best model saved with validation loss: 0.9176\n",
            "Epoch [11/20], Loss: 0.0231\n",
            "Validation Loss: 0.9798, Accuracy: 73.95%\n",
            "Epoch [12/20], Loss: 0.0231\n",
            "Validation Loss: 1.0432, Accuracy: 71.26%\n",
            "Epoch [13/20], Loss: 0.0873\n",
            "Validation Loss: 1.1530, Accuracy: 66.47%\n",
            "Epoch [14/20], Loss: 0.0083\n",
            "Validation Loss: 1.0776, Accuracy: 73.05%\n",
            "Epoch [15/20], Loss: 0.0043\n",
            "Validation Loss: 1.1237, Accuracy: 69.46%\n",
            "Epoch [16/20], Loss: 0.0066\n",
            "Validation Loss: 1.0213, Accuracy: 75.15%\n",
            "Epoch [17/20], Loss: 0.0029\n",
            "Validation Loss: 1.0333, Accuracy: 75.75%\n",
            "Epoch [18/20], Loss: 0.0039\n",
            "Validation Loss: 1.0506, Accuracy: 76.05%\n",
            "Epoch [19/20], Loss: 0.0028\n",
            "Validation Loss: 1.0796, Accuracy: 76.35%\n",
            "Epoch [20/20], Loss: 0.0019\n",
            "Validation Loss: 1.1078, Accuracy: 76.05%\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#start training\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "    val_loss = evaluate_model(model, valid_loader)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'gru_best_model.pth')\n",
        "        print(f\"Best model saved with validation loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPPPpCMd2b8C"
      },
      "source": [
        "Here you can get your final test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TMmgVhbB2cUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a80f2f-c4fa-4c16-c0ae-bd60d3205a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test set:\n",
            "Test Loss: 1.1396, Accuracy: 76.05%\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating on test set:\")\n",
        "test_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}